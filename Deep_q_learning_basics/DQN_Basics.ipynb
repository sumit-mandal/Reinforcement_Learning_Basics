{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537fc8e5-f551-4555-8726-281815fbe510",
   "metadata": {},
   "source": [
    "### Importing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a05ea35-d4da-4904-a7e7-b54ac46537ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Activation,Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import random \n",
    "import time\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import keras.backend.tensorflow_backend as backend\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf61af-8a6b-4b22-bf65-8113ae73b842",
   "metadata": {},
   "source": [
    "## Creating constatns and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d71f60a9-983e-4057-a194-f853eec4af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
    "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
    "MODEL_NAME = '2x256'\n",
    "MIN_REWARD = -200  # For model save\n",
    "MEMORY_FRACTION = 0.20\n",
    "\n",
    "# Environment settings\n",
    "EPISODES = 20_000\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "#  Stats settings\n",
    "AGGREGATE_STATS_EVERY = 50  # episodes\n",
    "SHOW_PREVIEW = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc24de-b9a0-496b-a12e-33ba27c2bd55",
   "metadata": {},
   "source": [
    "## Creating Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf943c65-25b2-4012-bf92-075050631fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlobEnv:\n",
    "    SIZE = 10\n",
    "    RETURN_IMAGES = True\n",
    "    MOVE_PENALTY = 1\n",
    "    ENEMY_PENALTY = 300\n",
    "    FOOD_REWARD = 25\n",
    "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)  # 4\n",
    "    ACTION_SPACE_SIZE = 9\n",
    "    PLAYER_N = 1  # player key in dict\n",
    "    FOOD_N = 2  # food key in dict\n",
    "    ENEMY_N = 3  # enemy key in dict\n",
    "    # the dict! (colors)\n",
    "    d = {1: (255, 175, 0),\n",
    "         2: (0, 255, 0),\n",
    "         3: (0, 0, 255)}\n",
    "\n",
    "    def reset(self):\n",
    "        self.player = Blob(self.SIZE)\n",
    "        self.food = Blob(self.SIZE)\n",
    "        while self.food == self.player:\n",
    "            self.food = Blob(self.SIZE)\n",
    "        self.enemy = Blob(self.SIZE)\n",
    "        while self.enemy == self.player or self.enemy == self.food:\n",
    "            self.enemy = Blob(self.SIZE)\n",
    "\n",
    "        self.episode_step = 0\n",
    "\n",
    "        if self.RETURN_IMAGES:\n",
    "            observation = np.array(self.get_image())\n",
    "        else:\n",
    "            observation = (self.player-self.food) + (self.player-self.enemy)\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        self.player.action(action)\n",
    "\n",
    "        #### MAYBE ###\n",
    "        #self.enemy.move()\n",
    "        #self.food.move()\n",
    "        ##############\n",
    "\n",
    "        if self.RETURN_IMAGES:\n",
    "            new_observation = np.array(self.get_image())\n",
    "        else:\n",
    "            new_observation = (self.player-self.food) + (self.player-self.enemy)\n",
    "\n",
    "        if self.player == self.enemy:\n",
    "            reward = -self.ENEMY_PENALTY\n",
    "        elif self.player == self.food:\n",
    "            reward = self.FOOD_REWARD\n",
    "        else:\n",
    "            reward = -self.MOVE_PENALTY\n",
    "\n",
    "        done = False\n",
    "        if reward == self.FOOD_REWARD or reward == -self.ENEMY_PENALTY or self.episode_step >= 200:\n",
    "            done = True\n",
    "\n",
    "        return new_observation, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((300, 300))  # resizing so we can see our agent in all its glory.\n",
    "        cv2.imshow(\"image\", np.array(img))  # show it!\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    # FOR CNN #\n",
    "    def get_image(self):\n",
    "        env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)  # starts an rbg of our size\n",
    "        env[self.food.x][self.food.y] = self.d[self.FOOD_N]  # sets the food location tile to green color\n",
    "        env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]  # sets the enemy location to red\n",
    "        env[self.player.x][self.player.y] = self.d[self.PLAYER_N]  # sets the player tile to blue\n",
    "        img = Image.fromarray(env, 'RGB')  # reading to rgb. Apparently. Even tho color definitions are bgr. ???\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7010bf3-e14c-47c9-8140-8a4447c229f2",
   "metadata": {},
   "source": [
    "## Environment Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ba5b1100-bd89-4820-8292-a93adf692f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlobEnv()\n",
    "\n",
    "# For stats\n",
    "ep_rewards = [-200]\n",
    "\n",
    "# For more repetitive results\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "# Create models folder\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec0528-acb7-4781-a7ec-a9f741f3ecf5",
   "metadata": {},
   "source": [
    "## Creating TensorBoard class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447369f4-6de5-4f81-aaa6-5264a3fa20a1",
   "metadata": {},
   "source": [
    "`By default tensorboard creates new log file for every .fit() call. We are changing tensorboard functionallity to only create a log file once`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fede8c4d-98b9-470e-9962-82929bdbe38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c1b1a-1f60-4de0-b298-94cfbeecdb5e",
   "metadata": {},
   "source": [
    "## Agent Class creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "21f668c0-32ba-4b40-9067-f67783655fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    #main model. #gets trained (.fit()) every step\n",
    "    def __init__(self):\n",
    "        #Main model\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        # Target model this is what we .predict against every step\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        # An array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        \n",
    "        #custom tensorboard-object\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir = f\"logs/{MODEL_NAME}-{int(time.time())}\")\n",
    "        \n",
    "        # Used to count when to update target network with main network's weights\n",
    "        self.target_update_counter = 0\n",
    "        \"\"\" we use \"self.target_update_counter\" to decide when it's time to update our target model (recall we decided update this model \n",
    "        every 'n' iterations, so that our predictions are reliable/stable).\"\"\"\n",
    "        \n",
    "            \n",
    "    \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(256,(3,3),input_shape=env.OBSERVATION_SPACE_VALUES))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(2,2))\n",
    "        model.add(Dropout(0.20))\n",
    "        \n",
    "        model.add(Conv2D(256,(3,3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(2,2))\n",
    "        model.add(Dropout(0.20))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64))\n",
    "        \n",
    "        model.add(Dense(env.ACTION_SPACE_SIZE,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    #Adds step's data to a memory replay array\n",
    "    # (observation space, action, reward, new observation space, done)\n",
    "    def update_replay_memory(self,transition):\n",
    "        self.replay_memory.append(transition)\n",
    "    #This just simply updates the replay memory, with the values commented above.\n",
    "    \n",
    "    \n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self,state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
    "    # So this is just doing a .predict(). We do the reshape because TensorFlow wants that \n",
    "    # exact explicit way to shape. The -1 just means a variable amount of this data will/could be fed through.\n",
    "    \n",
    "    # Trains main network every step during episode\n",
    "    def train(self,terminal_state,step):\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "    \n",
    "        \"\"\"Next, if we do have the proper amount of data to train, we need to randomly select the data we want \n",
    "        to train off of from our \"memory\" (which we'll wind up starting with using the past 1,000 steps, which \n",
    "        is the previous ~5-15ish episodes depending on how many steps are taken in the episode, but we'll eventually\n",
    "        populate this with up to 50,000 steps.\"\"\"\n",
    "    \n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory,MINIBATCH_SIZE)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        current_qs_list = self.model.predict(current_states) #mmake prediction on current states\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        #UPDATING THE MODEL\n",
    "        X = [] #future_sets. Images from the game\n",
    "        y = [] # labels or targets. Action we decide to take\n",
    "\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index,(current_state,action,reward,new_current_state,done) in enumerate(minibatch):\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT*max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "             # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "\n",
    "            # append to our training data\n",
    "            X.append(current_state) # immages. X are input in nn diagram\n",
    "            y.append(current_qs) #labels. y are output in nn diagram\n",
    "\n",
    "\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X)/255,np.array(y),batch_size = MINIBATCH_SIZE,verbose=0,\n",
    "                      shuffle = False,callbacks=[self.tensorboard] if terminal_state else None)\n",
    "        \n",
    "        # Updating to determine if we want to update target_model yet\n",
    "        \n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "            \n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "            \n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ea270-976b-45d3-aeca-78551e49307b",
   "metadata": {},
   "source": [
    "### Reason for using two Networks\n",
    "\n",
    "`Here, you can see there are apparently two models: self.model and self.target_model. What's going on here? So every step we take, we want to update Q values, but we also are trying to predict from our model. Especially initially, our model is starting off as random, and it's being updated every single step, per every single episode. What ensues here are massive fluctuations that are super confusing to our model. This is why we almost always train neural networks with batches (that and the time-savings). One way this is solved is through a concept of memory replay, whereby we actually have two models.`\n",
    "\n",
    "`The target_model is a model that we update every every n episodes (where we decide on n), and this the model that we use to determine what the future Q values.`\n",
    "\n",
    "`Once we get into working with and training these models, I will further point out how we're using these two models. Eventually, we converge the two models so they are the same, but we want the model that we query for future Q values to be more stable than the model that we're actively fitting every single step.`\n",
    "\n",
    "### One network predicts the appropriate action and the second network predicts the target Q values for finding the Bellman error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc41756-9b72-4c5b-95b4-4170cfc87ac7",
   "metadata": {},
   "source": [
    "### Bringing up Our BLOB class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2f8e516c-cdc5-479d-afc7-986b471dc434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blob:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.x = np.random.randint(0, size)\n",
    "        self.y = np.random.randint(0, size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Blob ({self.x}, {self.y})\"\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y)\n",
    "\n",
    "    def __eq__(self, other): #operator overloading Checking if two blobs are over each other\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def action(self, choice):\n",
    "        '''\n",
    "        Gives us 9 total movement options. (0,1,2,3,4,5,6,7,8)\n",
    "        '''\n",
    "        if choice == 0:\n",
    "            self.move(x=1, y=1)\n",
    "        elif choice == 1:\n",
    "            self.move(x=-1, y=-1)\n",
    "        elif choice == 2:\n",
    "            self.move(x=-1, y=1)\n",
    "        elif choice == 3:\n",
    "            self.move(x=1, y=-1)\n",
    "\n",
    "        elif choice == 4:\n",
    "            self.move(x=1, y=0)\n",
    "        elif choice == 5:\n",
    "            self.move(x=-1, y=0)\n",
    "\n",
    "        elif choice == 6:\n",
    "            self.move(x=0, y=1)\n",
    "        elif choice == 7:\n",
    "            self.move(x=0, y=-1)\n",
    "\n",
    "        elif choice == 8:\n",
    "            self.move(x=0, y=0)\n",
    "\n",
    "    def move(self, x=False, y=False):\n",
    "\n",
    "        # If no value for x, move randomly\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += x\n",
    "\n",
    "        # If no value for y, move randomly\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "\n",
    "        # If we are out of bounds, fix!\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > self.size-1:\n",
    "            self.x = self.size-1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > self.size-1:\n",
    "            self.y = self.size-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da86e6-5984-4c5c-b63b-3bbf081ad0ae",
   "metadata": {},
   "source": [
    "### Engaging our agent with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "580f5eab-d04c-4017-863d-d6a83bd966cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641707e-f17e-4693-b423-70e189cbc1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 1/20000 [00:00<4:59:14,  1.11episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sumit\\anaconda3\\envs\\tf1.5\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|2                                                                     | 70/20000 [09:20<75:38:25, 13.66s/episodes]"
     ]
    }
   ],
   "source": [
    "# Iterate over episodes\n",
    "for episode in tqdm(range(1,EPISODES + 1),ascii=True,unit='episodes'):\n",
    "    # Update tensorboard step every episode\n",
    "    agent.tensorboard.step = episode\n",
    "    \n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "    # Reset environment and get initial state\n",
    "    current_state = env.reset()\n",
    "    \n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    \n",
    "     # This part stays mostly the same, the change is to query a model for Q values\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "            \n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0,env.ACTION_SPACE_SIZE)\n",
    "            \n",
    "        new_state,reward,done = env.step(action)\n",
    "        \n",
    "        \n",
    "        \"\"\"Next, we account for the reward and render if it's time / and rendering is turned on.\n",
    "        Then, we update replay memory and finally run a .train(), which will fit our agent.model \n",
    "        every step, and then fit our agent.target_model if it has been 5 episodes. Then, we set our \n",
    "        current state to the new one, and add one step to our counter:\"\"\"\n",
    "        \n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
    "            env.render()\n",
    "            \n",
    "        # In Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state,action,reward,new_state,done))\n",
    "        agent.train(done,step)\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "        \n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "        \n",
    "        # Save model, but only when min reward is greater or equal a set value\n",
    "        if min_reward >= MIN_REWARD:\n",
    "            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86e44c-00c2-413b-bb9e-d232a0f7c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add DQN_Basics.ipynb\n",
    "! git commit -m \"21:10/29-05-2021\"\n",
    "! git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f45bd1-f2b6-4834-a6da-3a0c3766780d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.5]",
   "language": "python",
   "name": "conda-env-tf1.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
