{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a8a024-e941-46d3-a4e9-96fcebb9587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9e1186-f392-454e-874e-7c59918d7166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.observation_space.high [0.6  0.07]\n",
      "env.observation_space.low [-1.2  -0.07]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 25000\n",
    "SHOW_EVERY = 2000\n",
    "\n",
    "\n",
    "epsilon = 0.5\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES // 2\n",
    "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "\n",
    "# we can query the enviornment to find out the possible ranges for each of these state\n",
    "print(\"env.observation_space.high\",env.observation_space.high)\n",
    "print(\"env.observation_space.low\",env.observation_space.low)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2577fa8-dfbf-4ddc-b1a2-ca920b207407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space \n",
    "#Action 1 means push the car left action 2 means do nothing action 3 means push the car right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f49eede-794e-4eef-932e-5ef2011ff793",
   "metadata": {},
   "source": [
    " \n",
    "` For the value at index 0, we can see the high value is 0.6, the low is -1.2, and then for the value at index 1, the high is 0.07, and the low is -0.07. Okay, so these are the ranges, but from one of the above observation states that we output: [-0.27508804 -0.00268013], we can see that these numbers can become quite granular. Can you imagine the size of a Q Table if we were going to have a value for every combination of these ranges out to 8 decimal places? That'd be huge! And, more importantly, it'd be useless. We don't need that much granularity. So, instead, what we want to do is conver these continuous values to discrete values. Basically, we want to bucket/group the ranges into something more manageable.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffbc9819-bf27-46b3-b466-fd09ede8b27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09  0.007]\n"
     ]
    }
   ],
   "source": [
    "DISCRETE_OS_SIZE = [20]* len(env.observation_space.high) #observation size\n",
    "# size of len(env.observation_space.high) is 20 so it makes our DISCRETE_OS_SIZE = [20,20]\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
    "\n",
    "print(discrete_os_win_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e08ba70-0df7-4036-ac68-d9ccae6d3e3f",
   "metadata": {},
   "source": [
    "`It will tells us how large each bucket is, basically how much to increment the range by for each bucket`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3fa265-e8ee-4f92-bfaf-fc8e83f300dd",
   "metadata": {},
   "source": [
    "## Creating the Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1220d2-3700-46c5-926f-ad4efc46c12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_table.shape (20, 20, 3)\n"
     ]
    }
   ],
   "source": [
    "q_table = np.random.uniform(low=-2,high=0,size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "\n",
    "print(\"q_table.shape\",q_table.shape)\n",
    "\n",
    "#it makes q_table of the size (20,20,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0240e4d3-7f36-4ce9-9c8e-6a03fb47ebd1",
   "metadata": {},
   "source": [
    "`So, this is a 20x20x3 shape, which has initialized random Q values for us. The 20 x 20 bit is every combination of the bucket slices of all possible states. The x3 bit is for every possible action we could take.`\n",
    "\n",
    "`So these values are random, and the choice to be between -2 and 0 is also a variable. Each step is a -1 reward, and the flag is a 0 reward, so it seems to make sense to make the starting point of random Q values all negative.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce1f713-b2e5-4455-bc00-dedbc947c58d",
   "metadata": {},
   "source": [
    "## Next, we need a quick helper-function that will convert our environment \"state,\" which currently contains continuous values that would wind up making our Q-Table absolutely gigantic and take forever to learn.... to a \"discrete\" state instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ec77d8d-9746-4f6e-8ee8-b515f8658a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 10)\n"
     ]
    }
   ],
   "source": [
    "def get_discrete_state(state):\n",
    "    discrete_state = (state-env.observation_space.low)/ discrete_os_win_size\n",
    "    return tuple(discrete_state.astype(np.int32))\n",
    "\n",
    "discrete_state = get_discrete_state(env.reset())\n",
    "\n",
    "print(discrete_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417bd9b8-55ba-483c-a8fc-ae1ca8d048b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbca8c7-af16-415a-8eb5-b95336510c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "#iterating over episodes\n",
    "for episode in range(EPISODES):\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        print(episode)\n",
    "        render = True\n",
    "    else:\n",
    "        render = False\n",
    "        \n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "        \n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        if np.random.random() > epsilon:\n",
    "#         Next, we replace action = 2 with:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0,env.action_space.n)\n",
    "        new_state,reward,done,info = env.step(action) #every time we step with an action we get a new_state from the environment\n",
    "    #     print(f\"reward{reward}\\n new_state{new_state}\\n\")\n",
    "    # here state is it's poition and velocity\n",
    "\n",
    "#         Then, we want to grab the new discrete state:\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "        # If simulation did not end yet after last step - update Q table\n",
    "        if not done:\n",
    "            # Maximum possible Q value in next step (for new state)\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            # Current Q value (for current state and performed action)\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            \n",
    "            # And here's our equation for a new Q value for current state and action\n",
    "            new_q = (1 - LEARNING_RATE)*current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            \n",
    "            # Update Q table with new Q value\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "#             print(f\"we made it on episode {episode}\")\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "    #Now, we need to reset the discrete_state variable:\n",
    "        discrete_state = new_discrete_state\n",
    "        \n",
    "    if END_EPSILON_DECAYING >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65413b-5689-4d37-a6c3-a0131e58fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add learning_MountainCar.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2883496-5b21-4b93-a36d-c252fddc16bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.5]",
   "language": "python",
   "name": "conda-env-tf1.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
