{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a6dcee-4b75-42c9-a126-2829c26e2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c70737-c25e-42c9-a8cb-4ca59e259e03",
   "metadata": {},
   "source": [
    "`Next, we need to decide on an environment size. We're making a square grid here. As noted before, size will make a massive impact on our learning time.`\n",
    "\n",
    "`A 10x10 Q-Table for example, in this case, is ~15MB. A 20x20 is ~195MB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65325ba-084c-4543-b170-ca0ee2e7c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5273e0-ee5d-4c83-acf1-14ae95b33898",
   "metadata": {},
   "source": [
    "## Creating few other constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f63aa8e9-a330-4035-8ae0-abd35b9965bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "HM_EPISODES = 25000\n",
    "MOVE_PENALTY = 1\n",
    "ENEMY_PENALTY = 300\n",
    "FOOD_REWARD = 25\n",
    "\n",
    "epsilon = 0.1\n",
    "EPS_DECAY = 0.9998\n",
    "\n",
    "SHOW_EVERY = 1 #how often do we want to show or how often to play through env visually.\n",
    "\n",
    "start_q_table = \"qtable-1622223713.pickle\" # if we have a pickled Q table, we'll put the filename of it here.\n",
    "\n",
    "LEARNING_RATE = 0.1 #how quickly abandons current position in q-table for new value\n",
    "DISCOUNT = 0.95 #priority to future awards than present\n",
    "\n",
    "episode_rewards =  []\n",
    "\n",
    "PLAYER_N = 1\n",
    "FOOD_N = 2\n",
    "ENEMY_N = 3\n",
    "\n",
    "d = {1:(255,175,0), #player is blue\n",
    "     2:(0,255,0),#food is green\n",
    "     3:(0,0,255)} #Enemy is red\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64624fcb-c92c-486e-90af-f6aeda89a607",
   "metadata": {},
   "source": [
    "`Next this environment consists of blobs. These \"blobs\" are really just going to be squares, but, I am calling them blobs, alright?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1373232-af07-4578-8890-32a17ed2e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blob:\n",
    "    def __init__(self):\n",
    "        self.x = np.random.randint(0,SIZE)\n",
    "        self.y = np.random.randint(0,SIZE)\n",
    "        \"\"\"So, we'll begin by initializing the blobs randomly.\n",
    "        We could have the unfortunate environment where the enemy \n",
    "        and our blob, or the enemy and the food, are on the same \n",
    "        \"tile\" so to speak. Tough luck. For debugging purposes, \n",
    "        I wanted a string method:\"\"\"\n",
    "        \n",
    "    def __str__(self): #operator overloading\n",
    "        return f\"{self.x},{self.y}\"\n",
    "    \n",
    "    \"\"\"We need some sort of observation of our environment to use as our states.\n",
    "    I propose we simply pass the x and y deltas for the food and enemy to \n",
    "    our agent. To make this easy, I am going to override the - (subtraction)\n",
    "    operator, so we can just subtract two blobs from each other. \n",
    "    This method will look like:\"\"\"\n",
    "    \n",
    "    def __sub__(self,other): #operator overloading\n",
    "        return (self.x - other.x, self.y-other.y)\n",
    "    \n",
    "    \"\"\"Next, I'm going to add an \"action\" method, which will move based on \n",
    "    a \"discrete\" action being passed.\"\"\"\n",
    "    \n",
    "    def action(self,choice):\n",
    "        if choice == 0:\n",
    "            self.move(x=1,y=1)\n",
    "        elif choice == 1:\n",
    "            self.move(x=-1,y=-1)\n",
    "        elif choice ==2:\n",
    "            self.move(x=-1,y=1)\n",
    "        elif choice == 3:\n",
    "            self.move(x=1,y=-1)\n",
    "    \n",
    "    def move(self,x=False,y=False):\n",
    "        # If no value for x, move randomly\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1,2) #it will do -1,0 and 1\n",
    "        else:\n",
    "            self.x += x\n",
    "        # If no value for y, move randomly\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "            \n",
    "         # If we are out of bounds, fix!\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > SIZE-1:\n",
    "            self.x = SIZE-1\n",
    "            \n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > SIZE-1:\n",
    "            self.y = SIZE-1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a4c17-8263-4cd8-ab50-909abbf3d9b7",
   "metadata": {},
   "source": [
    "## Creating the q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ebf0ead-9e13-42fa-a64c-49b7fb80fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if start_q_table is None: #if we don't have start_q_table we create one\n",
    "    q_table = {}\n",
    "    \n",
    "    for x1 in range(-SIZE+1,SIZE):\n",
    "        for y1 in range(-SIZE+1,SIZE):\n",
    "            for x2 in range(-SIZE+1,SIZE):\n",
    "                for y2 in range(-SIZE+1,SIZE):\n",
    "                    q_table[((x1,y1),(x2,y2))]  = [np.random.uniform(-5,0) for i in range(4)]\n",
    "                    #key is tuples of tuples.\n",
    "                    #Each action space takes four random values. Because our action space is four\n",
    "                    \n",
    "else: #if we have q_table load old file\n",
    "    with open(start_q_table,\"rb\") as f:\n",
    "        q_table = pickle.load(f)\n",
    "        \n",
    "    \n",
    "#This isn't the most efficient code ever, but it should cover all \n",
    "#of our basis. Despite the table being pretty large for Q-Learning, \n",
    "#Python should still be able to make quick work of generating a table of this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3880ff-c782-4442-adbd-1c33f94e23d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.205209315522688, -2.688635439895376, -3.651877061837123, -4.418075098755273]\n"
     ]
    }
   ],
   "source": [
    "# For note, to look up on your Q-table, we would do something like:\n",
    "print(q_table[((-9, -2), (3,8))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad189d1-b5e5-46fd-a6fa-b174757fcb8d",
   "metadata": {},
   "source": [
    "## start iterating over episodes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479fbb9-5306-4034-89c2-1e592daef714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on # 0 , epsilon:0.1\n",
      "1 ep mean: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\envs\\tf1.5\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3420: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Sumit\\anaconda3\\envs\\tf1.5\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on # 1 , epsilon:0.09998000000000001\n",
      "1 ep mean: -85.0\n",
      "on # 2 , epsilon:0.09996000400000002\n",
      "1 ep mean: 5.0\n",
      "on # 3 , epsilon:0.09994001199920002\n",
      "1 ep mean: 17.0\n",
      "on # 4 , epsilon:0.09992002399680018\n",
      "1 ep mean: 14.0\n",
      "on # 5 , epsilon:0.09990003999200082\n",
      "1 ep mean: 23.0\n",
      "on # 6 , epsilon:0.09988005998400243\n",
      "1 ep mean: -200.0\n",
      "on # 7 , epsilon:0.09986008397200563\n",
      "1 ep mean: -43.0\n",
      "on # 8 , epsilon:0.09984011195521124\n",
      "1 ep mean: 24.0\n",
      "on # 9 , epsilon:0.0998201439328202\n",
      "1 ep mean: -470.0\n",
      "on # 10 , epsilon:0.09980017990403363\n",
      "1 ep mean: -87.0\n",
      "on # 11 , epsilon:0.09978021986805283\n",
      "1 ep mean: 18.0\n",
      "on # 12 , epsilon:0.09976026382407922\n",
      "1 ep mean: 4.0\n",
      "on # 13 , epsilon:0.09974031177131441\n",
      "1 ep mean: 9.0\n",
      "on # 14 , epsilon:0.09972036370896015\n",
      "1 ep mean: -63.0\n",
      "on # 15 , epsilon:0.09970041963621835\n",
      "1 ep mean: -2.0\n",
      "on # 16 , epsilon:0.09968047955229112\n",
      "1 ep mean: -46.0\n",
      "on # 17 , epsilon:0.09966054345638066\n",
      "1 ep mean: 2.0\n",
      "on # 18 , epsilon:0.09964061134768938\n",
      "1 ep mean: -18.0\n",
      "on # 19 , epsilon:0.09962068322541985\n",
      "1 ep mean: 11.0\n",
      "on # 20 , epsilon:0.09960075908877476\n",
      "1 ep mean: 14.0\n",
      "on # 21 , epsilon:0.099580838936957\n",
      "1 ep mean: -3.0\n",
      "on # 22 , epsilon:0.09956092276916961\n",
      "1 ep mean: -174.0\n",
      "on # 23 , epsilon:0.09954101058461579\n",
      "1 ep mean: 16.0\n",
      "on # 24 , epsilon:0.09952110238249887\n",
      "1 ep mean: 22.0\n",
      "on # 25 , epsilon:0.09950119816202237\n",
      "1 ep mean: 23.0\n",
      "on # 26 , epsilon:0.09948129792238997\n",
      "1 ep mean: 23.0\n",
      "on # 27 , epsilon:0.0994614016628055\n",
      "1 ep mean: -200.0\n",
      "on # 28 , epsilon:0.09944150938247294\n",
      "1 ep mean: -18.0\n",
      "on # 29 , epsilon:0.09942162108059645\n",
      "1 ep mean: -72.0\n",
      "on # 30 , epsilon:0.09940173675638032\n",
      "1 ep mean: 22.0\n",
      "on # 31 , epsilon:0.09938185640902905\n",
      "1 ep mean: 17.0\n",
      "on # 32 , epsilon:0.09936198003774725\n",
      "1 ep mean: 2.0\n",
      "on # 33 , epsilon:0.0993421076417397\n",
      "1 ep mean: 12.0\n",
      "on # 34 , epsilon:0.09932223922021136\n",
      "1 ep mean: 21.0\n",
      "on # 35 , epsilon:0.09930237477236732\n",
      "1 ep mean: 20.0\n",
      "on # 36 , epsilon:0.09928251429741285\n",
      "1 ep mean: -52.0\n",
      "on # 37 , epsilon:0.09926265779455337\n",
      "1 ep mean: -53.0\n",
      "on # 38 , epsilon:0.09924280526299446\n",
      "1 ep mean: -200.0\n",
      "on # 39 , epsilon:0.09922295670194187\n",
      "1 ep mean: 8.0\n",
      "on # 40 , epsilon:0.09920311211060148\n",
      "1 ep mean: -49.0\n",
      "on # 41 , epsilon:0.09918327148817936\n",
      "1 ep mean: 24.0\n",
      "on # 42 , epsilon:0.09916343483388172\n",
      "1 ep mean: -25.0\n",
      "on # 43 , epsilon:0.09914360214691495\n",
      "1 ep mean: -46.0\n",
      "on # 44 , epsilon:0.09912377342648557\n",
      "1 ep mean: 13.0\n",
      "on # 45 , epsilon:0.09910394867180028\n",
      "1 ep mean: 20.0\n",
      "on # 46 , epsilon:0.09908412788206593\n",
      "1 ep mean: 9.0\n",
      "on # 47 , epsilon:0.09906431105648952\n",
      "1 ep mean: -117.0\n",
      "on # 48 , epsilon:0.09904449819427823\n",
      "1 ep mean: 13.0\n",
      "on # 49 , epsilon:0.09902468929463937\n",
      "1 ep mean: -130.0\n",
      "on # 50 , epsilon:0.09900488435678044\n",
      "1 ep mean: -22.0\n",
      "on # 51 , epsilon:0.09898508337990909\n",
      "1 ep mean: -69.0\n",
      "on # 52 , epsilon:0.09896528636323311\n",
      "1 ep mean: -14.0\n",
      "on # 53 , epsilon:0.09894549330596046\n",
      "1 ep mean: -6.0\n",
      "on # 54 , epsilon:0.09892570420729926\n",
      "1 ep mean: 21.0\n",
      "on # 55 , epsilon:0.0989059190664578\n",
      "1 ep mean: -490.0\n",
      "on # 56 , epsilon:0.09888613788264451\n",
      "1 ep mean: -97.0\n",
      "on # 57 , epsilon:0.09886636065506799\n",
      "1 ep mean: 0.0\n",
      "on # 58 , epsilon:0.09884658738293697\n",
      "1 ep mean: -376.0\n",
      "on # 59 , epsilon:0.09882681806546038\n",
      "1 ep mean: -135.0\n",
      "on # 60 , epsilon:0.0988070527018473\n",
      "1 ep mean: -6.0\n",
      "on # 61 , epsilon:0.09878729129130692\n",
      "1 ep mean: 23.0\n",
      "on # 62 , epsilon:0.09876753383304866\n",
      "1 ep mean: 0.0\n",
      "on # 63 , epsilon:0.09874778032628205\n",
      "1 ep mean: -200.0\n",
      "on # 64 , epsilon:0.0987280307702168\n",
      "1 ep mean: -21.0\n",
      "on # 65 , epsilon:0.09870828516406276\n",
      "1 ep mean: -30.0\n",
      "on # 66 , epsilon:0.09868854350702995\n",
      "1 ep mean: -7.0\n",
      "on # 67 , epsilon:0.09866880579832854\n",
      "1 ep mean: -17.0\n",
      "on # 68 , epsilon:0.09864907203716888\n",
      "1 ep mean: 23.0\n",
      "on # 69 , epsilon:0.09862934222276146\n",
      "1 ep mean: 22.0\n",
      "on # 70 , epsilon:0.0986096163543169\n",
      "1 ep mean: 17.0\n",
      "on # 71 , epsilon:0.09858989443104604\n",
      "1 ep mean: 20.0\n",
      "on # 72 , epsilon:0.09857017645215983\n",
      "1 ep mean: 1.0\n",
      "on # 73 , epsilon:0.0985504624168694\n",
      "1 ep mean: -5.0\n",
      "on # 74 , epsilon:0.09853075232438603\n",
      "1 ep mean: -28.0\n",
      "on # 75 , epsilon:0.09851104617392116\n",
      "1 ep mean: -26.0\n",
      "on # 76 , epsilon:0.09849134396468638\n",
      "1 ep mean: -21.0\n"
     ]
    }
   ],
   "source": [
    "for episode in range(HM_EPISODES):\n",
    "    player = Blob()\n",
    "    food = Blob()\n",
    "    enemy = Blob()\n",
    "    \n",
    "    \"\"\"We will start tracking the episode rewards and their \n",
    "    improvements over time with episode_rewards. For each new episode, \n",
    "    we re-initialize the player, food, and enemy objects\"\"\"\n",
    "    \n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        print(f\"on # {episode} , epsilon:{epsilon}\")\n",
    "        print(f\"{SHOW_EVERY} ep mean: {np.mean(episode_rewards[-SHOW_EVERY:])}\")\n",
    "        show = True\n",
    "    else:\n",
    "        show = False\n",
    "        \n",
    "    # Now we start the actual frames/steps of the episode:\n",
    "    episode_reward = 0\n",
    "    for i in range(200):\n",
    "        obs = (player-food,player-enemy)\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[obs]) # macimum value of observation\n",
    "        else: \n",
    "            action = np.random.randint(0,4)\n",
    "            \n",
    "        # Take the action!\n",
    "        player.action(action) #we take the action based on past moves\n",
    "        \n",
    "        \n",
    "        #### later\n",
    "        # Adding movement to player\n",
    "        # and food\n",
    "        enemy.move()\n",
    "        food.move()\n",
    "        \n",
    "        #Codinng for reward purpose\n",
    "        if player.x == enemy.x and player.y == enemy.y:\n",
    "            reward = -ENEMY_PENALTY\n",
    "            \n",
    "        elif player.x == food.x and player.y == food.y:\n",
    "            reward = FOOD_REWARD\n",
    "        \n",
    "        else: \n",
    "            reward = -MOVE_PENALTY\n",
    "            \n",
    "        #work up our Q-table and Q-value information:\n",
    "        ## NOW WE KNOW THE REWARD, LET'S CALC YO\n",
    "        # first we need to obs immediately after the move.\n",
    "        new_obs = (player-food,player-enemy) #new observation\n",
    "        max_future_q = np.max(q_table[new_obs]) #max Q value for this new obs\n",
    "        current_q = q_table[obs][action] # current Q for our chosen action\n",
    "        \n",
    "        \n",
    "        #With these values, we can do our calculations:\n",
    "        if reward == FOOD_REWARD:\n",
    "            new_q = FOOD_REWARD\n",
    "        elif reward == -ENEMY_PENALTY:\n",
    "            new_q = - ENEMY_PENALTY\n",
    "        else:\n",
    "            new_q = (1- LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            \n",
    "        #updating the q_table\n",
    "        \n",
    "        q_table[obs][action] = new_q\n",
    "        \n",
    "        #Showing the environment\n",
    "        if show:\n",
    "            env = np.zeros((SIZE,SIZE,3),dtype=np.uint8) #((0,256),(0,256),3)  #starts an rbg of our size #it will be all black environment inintally\n",
    "            env[food.y][food.x] = d[FOOD_N] ## sets the food location tile to green color\n",
    "            env[player.y][player.x] = d[PLAYER_N]  # sets the player tile to blue\n",
    "            env[enemy.y][enemy.x] = d[ENEMY_N]  # sets the enemy location to red\n",
    "            \n",
    "            img = Image.fromarray(env,\"RGB\") ## reading to rgb. Apparently. Even tho color definitions are bgr. ???\n",
    "            img = img.resize((300,300)) #resizing so we can see our agent in all its glory.\n",
    "            cv2.imshow(\"\",np.array(img)) #show it\n",
    "            \n",
    "            if reward == FOOD_REWARD or reward == -ENEMY_PENALTY :# if player gets food or touches enemy\n",
    "                if cv2.waitKey(500) & 0xFF == ord(\"q\"): #wait for 500ms or press q\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"): #wait for 1 ms\n",
    "                    break\n",
    "                    \n",
    "        episode_reward += reward\n",
    "        if reward == FOOD_REWARD or reward == -ENEMY_PENALTY:\n",
    "            break\n",
    "                \n",
    "    episode_rewards.append(episode_reward)\n",
    "    epsilon *= EPS_DECAY\n",
    "\n",
    "moving_avg = np.convolve(episode_rewards, np.ones((SHOW_EVERY,))/SHOW_EVERY, mode='valid')\n",
    "\n",
    "plt.plot([i for i in range(len(moving_avg))], moving_avg)\n",
    "plt.ylabel(f\"Reward {SHOW_EVERY}moving average\")\n",
    "plt.xlabel(\"episode #\")\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "with open(f\"qtable-{int(time.time())}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b60dd50-c286-4f06-976c-78b764afd63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in Creating_Our_ENVIRONMENT/CreatingOwnEnvironment.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 44096a8] Testing the model,start_q_table changed(pickle inserted), epsilon and show_every changed\n",
      " 1 file changed, 574 insertions(+), 476 deletions(-)\n",
      " rewrite Creating_Our_ENVIRONMENT/CreatingOwnEnvironment.ipynb (61%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: unable to access 'https://github.com/sumit-mandal/Reinforcement_Learning_Basics.git/': Could not resolve host: github.com\n"
     ]
    }
   ],
   "source": [
    "! git add CreatingOwnEnvironment.ipynb\n",
    "! git commit -m \"Food and enemy movement added.See all the commits Testing the model,start_q_table changed(pickle inserted), epsilon and show_every changed\"\n",
    "! git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f4bc73-635e-4e13-bc5a-84b65695001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red is enemy \n",
    "# green is food\n",
    "#  blue is player\n",
    "\n",
    "# if we see blue and red really quickly we got to the food\n",
    "# if we see red and green we ran into enemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f33ce-9399-44f8-bbe7-e544470c773c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.5]",
   "language": "python",
   "name": "conda-env-tf1.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
